{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例介绍\n",
    "    泰坦尼克号的沉没是历史上伤亡人数最多、影响最深远的沉船事件之一。1912年4月15日，泰坦尼克号与冰山相撞，2224名乘客和船员中有1502人丧生。这场轰动性的悲剧震惊了国际社会，但也因此产生了更好的船舶安全法规。这次海难造成人员伤亡的原因之一是没有足够的救生艇供乘客和船员使用。\n",
    "    \n",
    "    虽然在沉船中幸存下来有一些运气因素，但有些人比其他人更可能存活下来。在本次学习中，在给定的部分存活人员信息中，将应用python来分析哪些乘客可能在悲剧中存活。此次，棕榈学院将携手Yiyu导师给各位想学习Python、想要在数据行业继续发展学习的同学来讲授如何完成这样一个project，相信会对你们的数据分析技能的提升大有裨益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四节课"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从sklearn中调用逻辑回归、决策树，随机森林包\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化模型，分别存入logr,dtree,rf变量中，并将变量置于名为models的列表中\n",
    "logr = LogisticRegression()\n",
    "dtree = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "models = [logr,dtree,rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这个函数调用逻辑回归模型来学习训练集和测试集\n",
    "logr.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据自变量xTest,xTrain分别利用训练好的模型预测出因变量\n",
    "y_pred_test = logr.predict(xTest)\n",
    "y_pred_train = logr.predict(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#算出测试集精确度（求均值）\n",
    "np.mean(y_pred_test == yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824438202247191"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#算出训练集精确度\n",
    "np.mean(y_pred_train == yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current model is LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "Training accuracy is 0.824438202247191\n",
      "\n",
      "Testing accuracy is 0.7821229050279329\n",
      "\n",
      "The current model is DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\n",
      "Training accuracy is 0.9873595505617978\n",
      "\n",
      "Testing accuracy is 0.7877094972067039\n",
      "\n",
      "The current model is RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Training accuracy is 0.9620786516853933\n",
      "\n",
      "Testing accuracy is 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "#对每一个模型，分别测试训练集和测试集的精确度\n",
    "for model in models:\n",
    "    print ('\\nThe current model is', model)\n",
    "    model.fit(xTrain, yTrain)\n",
    "    print ('\\nTraining accuracy is',np.mean(model.predict(xTrain) == yTrain))\n",
    "    print ('\\nTesting accuracy is',np.mean(model.predict(xTest) == yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current model is LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "Training accuracy is 0.8300561797752809\n",
      "\n",
      "Testing accuracy is 0.8156424581005587\n",
      "\n",
      "The current model is DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\n",
      "Training accuracy is 0.9873595505617978\n",
      "\n",
      "Testing accuracy is 0.7821229050279329\n",
      "\n",
      "The current model is RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Training accuracy is 0.9789325842696629\n",
      "\n",
      "Testing accuracy is 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "#对第二组数据进行相同的操作，每一个模型，分别测试训练集和测试集的精确度\n",
    "for model in models:\n",
    "    print ('\\nThe current model is', model)\n",
    "    model.fit(x2Train, y2Train)\n",
    "    print ('\\nTraining accuracy is',np.mean(model.predict(x2Train) == y2Train))\n",
    "    print ('\\nTesting accuracy is',np.mean(model.predict(x2Test) == y2Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#交叉验证是机器学习领域常用的验证模型是否优秀的方法；简而言之就是把数据切分成几个部分然后在训练集和测试集中交换使用\n",
    "#比如这次在训练集中用到的数据，下一次会放进测试集来使用，因此被称为 交叉\n",
    "#简单的交叉验证会直接按百分比来划分训练集和测试集，更难一些的方法是K-Fold，我们会使用这个方法来进行交叉验证\n",
    "#K-Fold其实就是把数据集切成K份，将每一份儿小数据集分别做一次验证集，每次剩下的K-1组份儿数据作为训练集，得到K个模型\n",
    "#\n",
    "#从sklearn中调用k次交叉验证包\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义k次交叉验证函数\n",
    "def CVKFold(k, X, y, Model):\n",
    "\n",
    "    # Random seed: reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # accuracy score \n",
    "    train_accuracy = [0 for i in range(k)] \n",
    "    test_accuracy = [0 for i in range(k)] \n",
    "   \n",
    "    # index\n",
    "    idx = 0\n",
    "    \n",
    "    # CV loop\n",
    "    kf = KFold(n_splits = k, shuffle = True)\n",
    "    \n",
    "    # Generate the sets\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Iteration number\n",
    "        #print(train_index,len(train_index))\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Calling the function/model\n",
    "               \n",
    "        if Model == \"Logit\":\n",
    "            clf = LogisticRegression(random_state = 0)\n",
    "                     \n",
    "        if Model == \"RForest\":\n",
    "            clf = RandomForestClassifier(random_state = 0)\n",
    "            \n",
    "        # Fit the model\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        \n",
    "        train_accuracy[idx] = np.mean(y_train_pred == y_train)\n",
    "        test_accuracy[idx] = np.mean(y_test_pred == y_test)\n",
    "        idx += 1\n",
    "\n",
    "    print (train_accuracy)\n",
    "    print (test_accuracy)\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8202247191011236, 0.8017456359102244, 0.8154613466334164, 0.8216957605985037, 0.8054862842892768, 0.8104738154613467, 0.8042394014962594, 0.8104738154613467, 0.8092269326683291, 0.816708229426434]\n",
      "[0.7555555555555555, 0.8539325842696629, 0.7752808988764045, 0.7415730337078652, 0.8539325842696629, 0.7865168539325843, 0.8876404494382022, 0.797752808988764, 0.8314606741573034, 0.797752808988764]\n"
     ]
    }
   ],
   "source": [
    "#应用逻辑回归模型，将数据分为10份，每次取一个样本作为验证数据，剩下k-1个样本作为训练数据\n",
    "train_acc,test_acc = CVKFold(10,all_x,y,\"Logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.811573594104626, 0.8081398252184769)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#验证训练数据和测试数据的精确度\n",
    "np.mean(train_acc),np.mean(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9650436953807741, 0.9688279301745636, 0.9763092269326683, 0.9763092269326683, 0.972568578553616, 0.9738154613466334, 0.9713216957605985, 0.972568578553616, 0.9750623441396509, 0.9738154613466334]\n",
      "[0.7555555555555555, 0.7865168539325843, 0.7191011235955056, 0.7865168539325843, 0.8426966292134831, 0.8089887640449438, 0.8314606741573034, 0.8202247191011236, 0.8876404494382022, 0.797752808988764]\n"
     ]
    }
   ],
   "source": [
    "#应用随机森林模型，将数据分为10份，每次取一个样本作为验证数据，剩下k-1个样本作为训练数据\n",
    "train_acc,test_acc = CVKFold(10,all_x,y,\"RForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
